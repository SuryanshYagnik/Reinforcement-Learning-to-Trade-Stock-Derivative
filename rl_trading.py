# -*- coding: utf-8 -*-
"""RL_trading.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TlhMfRDx2D4gMIY5mvvwRwnMAW-BoK-b

**Welcome to my Colab notebook on Implementing Strangle options trading using reinforcement learning!**

-Explore the fascinating world of financial markets.

-Particularly focusing on options trading strategies.

**Options trading provides a unique opportunity for investors:**
-Manage risk.

-Speculate on price movements.

-Generate income.

**Leveraging the power of reinforcement learning:**

-Aim to develop intelligent trading agents.

-Capable of making optimal decisions in dynamic market environments.

**-Key Steps of This Notebook:**

a. Explore the Fundamentals of Options Trading:
Understand the basics of options trading.

b. Understand the Nuances of Market Data Processing:
Learn techniques for processing market data.

c. Design Custom Reinforcement Learning Environments:
Tailored to options trading scenarios.

d. Train Policy Networks:
Using reinforcement learning techniques.
Enable informed trading decisions.

**End Goal:**
Gain a deeper understanding of options trading.
Learn how to apply cutting-edge techniques from artificial intelligence.
Enhance your trading strategies

**Overview of Coding Steps:**

Data Preparation:
Import necessary libraries for data manipulation.
Load historical options trading data.
Preprocess the data, including cleaning and formatting.

Exploratory Data Analysis (EDA):
Visualize key metrics and distributions of the options trading data.
Identify patterns, trends, and anomalies.

Feature Engineering:
Create relevant features from the raw data to capture important information for the trading model.
Transform the data into a format suitable for reinforcement learning.

Reinforcement Learning Environment Design:
Define the state space, action space, and reward structure for the trading environment.
Implement custom trading environment classes compatible with reinforcement learning libraries.
Policy Network Architecture:
Design the neural network architecture for the policy network.
Choose appropriate activation functions, layers, and optimizer for training.

Training the Policy Network:
Split the data into training and validation sets.
Train the policy network using reinforcement learning algorithms like Q-learning or Deep Q-Networks (DQN).

Evaluation and Fine-Tuning:
Evaluate the trained model on unseen data to assess its performance.
Fine-tune hyperparameters or adjust the model architecture based on evaluation results.

Deployment and Testing:
Deploy the trained model to make real-time trading decisions.
Test the model in live or simulated trading environments to validate its effectiveness.

**Nessary Libraries Installing**
"""

!pip install pandas_ta
!pip install stable-baselines3
!pip install ta
!pip install mplfinance

"""**Importing Libraries**"""

import numpy as np
import pandas as pd
import gym
import mplfinance as mpf
import yfinance as yf
from gym import spaces
import numpy as np
import pandas as pd
import pandas_ta as ta
import torch
import torch.nn as nn
import torch.nn.functional as F
from stable_baselines3 import PPO
import pandas_ta as ta
from ta import add_all_ta_features
import torch.optim as optim
from torch.distributions import Categorical

"""**DATA GATHERING AND PREPROCESSING**

Stock Historical *Data*
"""

# Fetch historical stock price data for Axis Bank
axis_data = yf.download('AXISBANK.NS', start='2024-01-01', end='2024-04-01')

# Display the first few rows of the data
print(axis_data.head())

# Plot candlestick chart
mpf.plot(axis_data, type='candle', style='charles',
         volume=True, title='Axis Bank Candlestick Chart',
         ylabel='Price', ylabel_lower='Volume', show_nontrading=True)

"""Option Chain Data"""

option_chain = pd.read_csv("/content/option-chain-1715012833554.csv")
print(option_chain.head())

print(option_chain.info())

"""Handling Missing Values"""

# Check for missing values in the historical stock data
missing_values_stock = axis_data.isnull().sum()
print("Missing values in historical stock data:")
print(missing_values_stock)

# Check for missing values in the option chain data
missing_values_option_chain = option_chain.isnull().sum()
print("Missing values in option chain data:")
print(missing_values_option_chain)

option_chain['Greeks: Put Rho'].fillna(method='ffill', inplace=True)

"""**Setting Trading Invironment Using Open AI gym**

I've created a Gym environment for trading. In the initialization, I set up the initial balance, data, action space (for buying, selling, or holding), and observation space (based on the data columns).

The reset method initializes the environment back to its starting state, including resetting the balance, current step, positions, net worth, and state.

In the step method, I execute the chosen action (buy, sell, or hold), updating the environment accordingly. Buying deducts from the balance and adds to positions, selling does the opposite, and holding doesn't change anything. I calculate rewards based on the action and price changes.

The render method prints out the current step, balance, and net worth for monitoring the agent's performance.
"""

class TradingEnvironment(gym.Env):
    def __init__(self, initial_balance, data):
        super(TradingEnvironment, self).__init__()
        self.initial_balance = initial_balance
        self.balance = initial_balance
        self.data = data
        self.action_space = spaces.Discrete(3)  # 3 actions: Buy, Sell, Hold
        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(len(data.columns),))

    def reset(self):
        self.balance = self.initial_balance
        self.current_step = 0
        self.positions = []
        self.net_worth = [self.balance]
        self.state = self.data.iloc[self.current_step].values
        return self.state

    def step(self, action):
        # Execute action
        if action == 0:  # Buy
            self.positions.append(self.data.iloc[self.current_step])
            self.balance -= self.data.iloc[self.current_step]
        elif action == 1:  # Sell
            if len(self.positions) == 0:
                reward = -10  # Penalty for selling without holding
            else:
                bought_price = self.positions.pop(0)
                reward = self.data.iloc[self.current_step] - bought_price
                self.balance += self.data.iloc[self.current_step]
        else:  # Hold
            pass

        # Move to the next time step
        self.current_step += 1

        # Update net worth
        net_worth = self.balance + sum(self.positions)

        # Get the next state
        if self.current_step == len(self.data):
            done = True
            next_state = None
        else:
            next_state = self.data.iloc[self.current_step].values
            done = False

        return next_state, reward, done, {}

    def render(self, mode='human'):
        print(f'Step: {self.current_step}, Balance: {self.balance}, Net Worth: {self.net_worth[-1]}')

"""**RL Model Intialization**

This code defines a Gym environment for trading strangles. It initializes with asset price, option chain, portfolio positions, and technical indicators. The observation space is a dictionary containing these components. The reset method resets the environment, and the step method executes actions and returns the next state, reward, and done flag
"""

class StrangleTradingEnvironment(gym.Env):
    def __init__(self, asset_price, option_chain, portfolio_positions, technical_indicators):
        self.asset_price = asset_price
        self.option_chain = option_chain
        self.portfolio_positions = portfolio_positions
        self.technical_indicators = technical_indicators

        # Define the state space
        asset_price_space = spaces.Box(low=0, high=float('inf'), shape=(1,), dtype=float)
        option_chain_space = spaces.Box(low=0, high=float('inf'), shape=(len(option_chain.columns),), dtype=float)
        portfolio_positions_space = spaces.Box(low=0, high=float('inf'), shape=(len(portfolio_positions),), dtype=int)
        technical_indicators_space = spaces.Box(low=0, high=float('inf'), shape=(len(technical_indicators),), dtype=float)

        self.observation_space = spaces.Dict({
            'asset_price': asset_price_space,
            'option_chain': option_chain_space,
            'portfolio_positions': portfolio_positions_space,
            'technical_indicators': technical_indicators_space
        })

    def reset(self):
        # Reset the environment to its initial state
        pass

    def step(self, action):
        # Execute the given action and return the next state, reward, and done flag
        pass

"""**DEFINING STATE SPACE**

-ASSET PRICE

-OPTION CHAIN

-PORTFOLIO POSITION

-TECHNICAL INDICATOR
"""

class StrangleTradingEnvironment(gym.Env):
    def __init__(self, asset_price, option_chain, portfolio_positions):
        self.asset_price = asset_price
        self.option_chain = option_chain
        self.portfolio_positions = portfolio_positions

        # Define the state space
        asset_price_space = spaces.Box(low=0, high=float('inf'), shape=(1,), dtype=float)
        option_chain_space = spaces.Box(low=0, high=float('inf'), shape=(len(option_chain.columns),), dtype=float)
        portfolio_positions_space = spaces.Box(low=0, high=float('inf'), shape=(len(portfolio_positions),), dtype=int)

        # Calculate technical indicators
        asset_data = pd.DataFrame({'Close': asset_price})
        asset_data.ta.rsi(length=14, append=True)
        asset_data.ta.macd(append=True)
        asset_data.ta.bbands(append=True)
        asset_data.ta.adx(append=True)

        # Extract technical indicators
        technical_indicators = asset_data.iloc[-1][['RSI_14', 'MACDh_12_26_9', 'MACDs_12_26_9', 'BBL_5_2.0', 'BBM_5_2.0', 'BBU_5_2.0', 'ADX_14']]

        technical_indicators_space = spaces.Box(low=0, high=float('inf'), shape=(len(technical_indicators),), dtype=float)

        self.observation_space = spaces.Dict({
            'asset_price': asset_price_space,
            'option_chain': option_chain_space,
            'portfolio_positions': portfolio_positions_space,
            'technical_indicators': technical_indicators_space
        })

    def reset(self):
        # Reset the environment to its initial state
        pass

    def step(self, action):
        # Execute the given action and return the next state, reward, and done flag
        pass

"""
This StrangleTradingEnvironment class initializes a Gym environment for trading strangles. It defines observation spaces for asset price, option chain, portfolio positions, and technical indicators.

Additionally, it sets up a discrete **Action space** with four actions:

-Enter

 -Adjust

 -Hold

 -Exit"""

class StrangleTradingEnvironment(gym.Env):
    def __init__(self, asset_price, option_chain, portfolio_positions):
        self.asset_price = asset_price
        self.option_chain = option_chain
        self.portfolio_positions = portfolio_positions

        # Define the state space
        asset_price_space = spaces.Box(low=0, high=float('inf'), shape=(1,), dtype=float)
        option_chain_space = spaces.Box(low=0, high=float('inf'), shape=(len(option_chain.columns),), dtype=float)
        portfolio_positions_space = spaces.Box(low=0, high=float('inf'), shape=(len(portfolio_positions),), dtype=int)

        # Calculate technical indicators
        asset_data = pd.DataFrame({'Close': asset_price})
        asset_data.ta.rsi(length=14, append=True)
        asset_data.ta.macd(append=True)
        asset_data.ta.bbands(append=True)
        asset_data.ta.adx(append=True)

        # Extract technical indicators
        technical_indicators = asset_data.iloc[-1][['RSI_14', 'MACDh_12_26_9', 'MACDs_12_26_9', 'BBL_5_2.0', 'BBM_5_2.0', 'BBU_5_2.0', 'ADX_14']]
        technical_indicators_space = spaces.Box(low=0, high=float('inf'), shape=(len(technical_indicators),), dtype=float)

        self.observation_space = spaces.Dict({
            'asset_price': asset_price_space,
            'option_chain': option_chain_space,
            'portfolio_positions': portfolio_positions_space,
            'technical_indicators': technical_indicators_space
        })

        # Define the action space
        self.action_space = spaces.Discrete(4)  # 4 possible actions: Enter, Adjust, Hold, Exit

"""**DEFINING ACTION SPACE BASED ON STRANGLE TRADINF STRATEGY**
This Environment class models a trading environment for managing strangle positions. Here's a quick overview:

Actions:
There are four possible actions: Enter Strangle, Adjust Position, Hold Position, and Exit Position, each represented by a numeric constant.

Initialization:
It initializes the environment with available funds, portfolio positions (both call and put options), strangle cost, and strangle code.

Step Method:
It executes the given action and returns the next state, reward, and a boolean indicating if the episode is done.
The action determines whether to enter, adjust, hold, or exit the strangle position.

Internal Methods:
_enter_strangle_position(): Enters a new strangle position if conditions are met.
_adjust_position(): Adjusts the existing strangle position.
_exit_position(): Exits the current strangle position if conditions are met.

_calculate_reward(): Calculates the reward based on the action taken.
_update_state(): Updates the state of the environment, such as asset price and option chain.

_get_next_state(): Returns the next state.
This class provides a framework for managing strangle positions within a trading environment, allowing for actions to be taken and rewards to be calculated accordingly.

Long Strangle
"""

class Environment:
    ACTION_ENTER_STRANGLE = 0
    ACTION_ADJUST_POSITION = 1
    ACTION_HOLD_POSITION = 2
    ACTION_EXIT_POSITION = 3

    def __init__(self):
        self.available_funds = 0
        self.portfolio_positions = {'call_option': 0, 'put_option': 0}
        self.strangle_cost = 0
        self.strangle_code = 0

    def step(self, action):
        reward = 0
        done = False
        next_state = {}

        if action == self.ACTION_ENTER_STRANGLE:
            self._enter_strangle_position()
            reward = self._calculate_reward(action)

        elif action == self.ACTION_ADJUST_POSITION:
            self._adjust_position()
            reward = self._calculate_reward(action)

        elif action == self.ACTION_HOLD_POSITION:
            # Do nothing, maintain the current portfolio positions
            reward = self._calculate_reward(action)

        elif action == self.ACTION_EXIT_POSITION:
            self._exit_position()
            reward = self._calculate_reward(action)
            done = True

        # Update the state of the environment
        self._update_state()
        next_state = self._get_next_state()

        return next_state, reward, done, {}

    def _enter_strangle_position(self):
        if self.available_funds >= self.strangle_cost and self.portfolio_positions['call_option'] == 0 and self.portfolio_positions['put_option'] == 0:
            self.available_funds -= self.strangle_cost
            self.portfolio_positions['call_option'] += 1
            self.portfolio_positions['put_option'] += 1

    def _adjust_position(self):
        if self.portfolio_positions['call_option'] == 1 and self.portfolio_positions['put_option'] == 1:
            # Modify the existing strangle position
            # Update portfolio positions accordingly
            pass  # Implement the logic to adjust the position

    def _exit_position(self):
        if self.portfolio_positions['call_option'] == 1 and self.portfolio_positions['put_option'] == 1:
            self.available_funds += self.strangle_code
            self.portfolio_positions['call_option'] -= 1
            self.portfolio_positions['put_option'] -= 1

    def _calculate_reward(self, action):
        # Calculate reward based on the action
        # This depends on your specific reward function
        pass

    def _update_state(self):
        # Update the state of the environment
        # This involves updating asset price, option chain, etc.
        pass

    def _get_next_state(self):
        # Return the next state
        pass

"""
This calculate_reward function calculates the **reward** based on the action taken, the current portfolio value, and the previous portfolio value."""

def calculate_reward(action, portfolio_value, previous_portfolio_value):
    # Calculate the reward based on the action and portfolio value
    # Consider factors like profitability, risk control, time decay, and transaction costs
    reward = 0

    # Profitability: Reward the agent for profitable trades
    if action == ENTER_POSITION:
        # Calculate the difference between current portfolio value and previous portfolio value
        profit = portfolio_value - previous_portfolio_value
        reward += profit

    # Risk Control: Penalize the agent for excessive risk
    # Implement risk control measures like maximum drawdown or risk-adjusted returns
    # For example, penalize based on the maximum drawdown
    max_drawdown_penalty = max(0, previous_portfolio_value - portfolio_value)
    reward -= max_drawdown_penalty

    # Time Decay: Penalize the agent for holding options too close to expiration
    # For example, penalize based on the time to expiration of the options

    # Transaction Costs: Deduct transaction costs from the reward
    # For example, deduct a fixed transaction cost for each trade

    return reward

"""**PolicyNetwork** architecture allows you to map the current state of the environment (such as market conditions or portfolio status) to a probability distribution over possible actions (like entering, adjusting, holding, or exiting positions). By training this network, your strategy can learn to make decisions based on past experiences and maximize long-term rewards in the trading environment."""

class PolicyNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.softmax(x, dim=-1)

"""Short Strangle"""

import gym
import numpy as np
import pandas as pd

class StrangleTradingEnvironment(gym.Env):
    def __init__(self, asset_price, option_chain, portfolio_positions, transaction_cost=0.01, start_date=None, end_date=None):
        self.asset_price = asset_price
        self.option_chain = option_chain
        self.portfolio_positions = portfolio_positions
        self.transaction_cost = transaction_cost

        # No need to download data if asset_price is provided directly
        if not start_date and not end_date:
            pass  # Use provided asset_price
        else:
            raise ValueError("Downloading live Axis Bank data is not currently supported. Please provide pre-defined asset price data.")

        # Define the observation space (replace with your actual elements and data types)
        observation_space = spaces.Dict({
            'asset_price': spaces.Box(low=0, high=float('inf'), shape=(len(self.asset_price.columns),), dtype=float),
            'option_chain': spaces.Box(low=0, high=float('inf'), shape=(len(self.option_chain.columns),), dtype=float),
            'portfolio_positions': spaces.Box(low=0, high=float('inf'), shape=(len(portfolio_positions),), dtype=int)
        })
        self.observation_space = observation_space

        # Define the action space (buy call, buy put, sell all, hold)
        self.action_space = spaces.Discrete(4)

        # Reset the environment for the first step
        self.reset()

    def reset(self):
        # Reset the environment to its initial state
        initial_state = {
            'asset_price': self.asset_price.iloc[0].values,  # Keep this line to access DataFrame values
            'option_chain': self.option_chain.iloc[0].values,  # Keep this line to access DataFrame values
            'portfolio_positions': self.portfolio_positions  # Use the list directly here
        }
        self.current_state = initial_state
        return initial_state



    def step(self, action):
        # Update state and calculate reward based on action
        next_state = self.current_state.copy()
        reward = 0
        done = False


        # Get current option prices (replace with your option pricing logic)
        current_call_price = self.option_chain[self.option_chain['Strike Call'] == next_state['option_chain'][4]]['Price'].iloc[0]
        current_put_price = self.option_chain[self.option_chain['Strike Put'] == next_state['option_chain'][1]]['Price'].iloc[0]

        # Perform action and update portfolio positions
        if action == 0:  # Hold
            pass
        elif action == 1:  # Buy call option
            # Check if enough cash to buy call
            if next_state['portfolio_positions'][0] >= current_call_price + self.transaction_cost:
                next_state['portfolio_positions'][0] -= current_call_price + self.transaction_cost
                next_state['portfolio_positions'][2] += 1  # Increase call option position
                reward = -current_call_price  # Initial cost of buying the call
            else:
                reward = -10  # Penalty for not having enough cash
        elif action == 2:  # Buy put option
            # Check if enough cash to buy put
            if next_state['portfolio_positions'][0] >= current_put_price + self.transaction_cost:
                next_state['portfolio_positions'][0] -= current_put_price + self.transaction_cost
                next_state['portfolio_positions'][3] += 1  # Increase put option position
                reward = -current_put_price  # Initial cost of buying the put
            else:
                reward = -10  # Penalty for not having enough cash
        elif action == 3:  # Sell all options
            # Sell call option (if any)
            if next_state['portfolio_positions'][2] > 0:
                reward += next_state['portfolio_positions'][2] * current_call_price - self.transaction_cost * next_state['portfolio_positions'][2]
                next_state['portfolio_positions'][0] += next_state['portfolio_positions'][2] * current_call_policy  # Corrected line
            # Sell put option (if any)
            if next_state['portfolio_positions'][3] > 0:
                reward += next_state['portfolio_positions'][3] * current_put_price - self.transaction_cost * next_state

def step(self, action):
  # Update state and calculate reward based on action
  next_state = self.current_state.copy()
  reward = 0
  done = False

  # Get current asset price
  current_asset_price = next_state['asset_price'][0]

  # Calculate portfolio value at previous step
  previous_portfolio_value = self.portfolio_positions[0] + \
                             self.portfolio_positions[2] * self.option_chain.iloc[self.current_step]['Price'].iloc[4] + \
                             self.portfolio_positions[3] * self.option_chain.iloc[self.current_step]['Price'].iloc[1]

  # Calculate portfolio value at current step
  current_portfolio_value = next_state['portfolio_positions'][0] + \
                             next_state['portfolio_positions'][2] * current_call_price + \
                             next_state['portfolio_positions'][3] * current_put_price

  # Reward for profitability
  reward += (current_portfolio_value - previous_portfolio_value)

  # Penalty for large drawdowns (replace with a proper risk metric)
  if current_portfolio_value < 0.9 * previous_portfolio_value:
      reward -= abs(current_portfolio_value - previous_portfolio_value)

  # Time decay factor (adjust based on option expiry)
  time_decay = 1 - (self.option_chain.iloc[self.current_step]['Expiry Date'] - self.current_step) / self.option_chain.iloc[0]['Days to Expiry']

  # Apply time decay to reward
  reward *= time_decay

  # Calculate portfolio return
  portfolio_return = (current_portfolio_value - previous_portfolio_value) / previous_portfolio_value

  # Daily volatility (replace with your volatility calculation)
  daily_volatility = ...  # Implement your volatility calculation

  # Calculate Sharpe Ratio
  sharpe_ratio = portfolio_return / daily_volatility

  # Penalty for low Sharpe Ratio (adjust threshold)
  if sharpe_ratio < 0.5:
      reward -= abs(sharpe_ratio - 0.5)

  # ... rest of the code for handling actions and updating positions ...

  # Update current state and step counter
  self.current_state = next_state
  self.current_step += 1

  return next_state, reward, done

"""**Training** **Agent**"""

def train_agent(env, policy_net, num_steps=100):
    model = PPO(policy_net, env, verbose=1)
    model.learn(total_timesteps=num_steps)
    return model

"""This code sets up a basic structure for training an agent using reinforcement learning:

-It defines a placeholder PolicyNetwork class and a placeholder training function, train_agent.

-Historical stock price and option chain data are fetched and preprocessed.

-The environment is instantiated with the processed data.

-The train_agent function is called with the environment and PolicyNetwork class as arguments, along with the number of training steps.

This code defines a Proximal Policy Optimization (PPO) agent (PPOAgent) and a policy network (PolicyNetwork) using PyTorch for reinforcement learning tasks. The agent selects actions based on the policy network's output probabilities and updates its parameters using the PPO algorithm.
"""

class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return F.softmax(x, dim=-1)

class PPOAgent:
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, clip_ratio=0.2, value_coef=0.5, entropy_coef=0.01):
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        self.gamma = gamma
        self.clip_ratio = clip_ratio
        self.value_coef = value_coef
        self.entropy_coef = entropy_coef

    def select_action(self, state):
        state_tensor = torch.FloatTensor(state)
        action_probs = self.policy(state_tensor)
        dist = Categorical(action_probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        return action.item(), log_prob

    def update(self, states, actions, log_probs, rewards, dones, next_states):
        states_tensor = torch.FloatTensor(states)
        actions_tensor = torch.LongTensor(actions)
        log_probs_tensor = torch.FloatTensor(log_probs)
        rewards_tensor = torch.FloatTensor(rewards)
        dones_tensor = torch.FloatTensor(dones)
        next_states_tensor = torch.FloatTensor(next_states)

        # Compute value estimates
        values = self.policy(states_tensor).gather(1, actions_tensor.unsqueeze(1)).squeeze()
        next_values = self.policy(next_states_tensor).max(1)[0]
        td_targets = rewards_tensor + self.gamma * next_values * (1 - dones_tensor)
        td_errors = td_targets - values.detach()

        # Compute policy loss
        action_probs = self.policy(states_tensor)
        dist = Categorical(action_probs)
        new_log_probs = dist.log_prob(actions_tensor)
        ratio = (new_log_probs - log_probs_tensor).exp()
        surr1 = ratio * td_errors
        surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * td_errors
        policy_loss = -torch.min(surr1, surr2).mean()

        # Compute value loss
        value_loss = F.smooth_l1_loss(values, td_targets.detach())

        # Compute entropy loss
        entropy_loss = dist.entropy().mean()

        # Compute total loss
        loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy_loss

        # Optimize the model
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

"""Adjusting data for agent training"""

import numpy as np
import pandas as pd
import gym
from gym import spaces
import torch
import torch.nn as nn
import torch.nn.functional as F
from stable_baselines3 import PPO
from stable_baselines3.ppo import MlpPolicy

class StrangleTradingEnvironment(gym.Env):
  def __init__(self, asset_price, option_chain, portfolio_positions, transaction_cost=0.01):
    self.asset_price = asset_price
    self.option_chain = option_chain
    self.portfolio_positions = portfolio_positions
    self.transaction_cost = transaction_cost  # Per-transaction cost

    # Define the state space
    asset_price_space = spaces.Box(low=0, high=float('inf'), shape=(len(asset_price.columns),), dtype=float)
    option_chain_space = spaces.Box(low=0, high=float('inf'), shape=(len(option_chain.columns),), dtype=float)
    portfolio_positions_space = spaces.Box(low=0, high=float('inf'), shape=(len(portfolio_positions),), dtype=int)

    self.observation_space = spaces.Dict({
        'asset_price': asset_price_space,
        'option_chain': option_chain_space,
        'portfolio_positions': portfolio_positions_space
    })

    # Define the action space (buy call, buy put, sell all, hold)
    self.action_space = spaces.Discrete(4)

  def reset(self):
    # Reset the environment to its initial state
    initial_state = {
      'asset_price': self.asset_price.iloc[0].values,
      'option_chain': self.option_chain.iloc[0].values,
      'portfolio_positions': list(self.portfolio_positions.values())
    }
    return initial_state

  def step(self, action):
    # Update state and calculate reward based on action
    next_state = self.current_state.copy()
    reward = 0
    done = False

    # Get current option prices (replace with your option pricing logic)
    current_call_price = self.option_chain[self.option_chain['Strike Call'] == next_state['option_chain'][4]]['Price'].iloc[0]
    current_put_price = self.option_chain[self.option_chain['Strike Put'] == next_state['option_chain'][1]]['Price'].iloc[0]

    # Perform action and update portfolio positions
    if action == 0:  # Hold
        pass
    elif action == 1:  # Buy call option
        # Check if enough cash to buy call
        if next_state['portfolio_positions'][0] >= current_call_price + self.transaction_cost:
            next_state['portfolio_positions'][0] -= current_call_price + self.transaction_cost
            next_state['portfolio_positions'][2] += 1  # Increase call option position
            reward = -current_call_price  # Initial cost of buying the call
        else:
            reward = -10  # Penalty for not having enough cash
    elif action == 2:  # Buy put option
        # Check if enough cash to buy put
        if next_state['portfolio_positions'][0] >= current_put_price + self.transaction_cost:
            next_state['portfolio_positions'][0] -= current_put_price + self.transaction_cost
            next_state['portfolio_positions'][3] += 1  # Increase put option position
            reward = -current_put_price  # Initial cost of buying the put
        else:
            reward = -10  # Penalty for not having enough cash
    elif action == 3:  # Sell all options
        # Sell call option (if any)
        if next_state['portfolio_positions'][2] > 0:
            reward += next_state['portfolio_positions'][2] * current_call_price - self.transaction_cost * next_state['portfolio_positions'][2]
            next_state['portfolio_positions'][0] += next_state['portfolio_positions'][2] * current_call_price - self.transaction_cost * next_state['portfolio_positions'][2]
            next_state['portfolio']

import numpy as np
import pandas as pd
import gym
import yfinance as yf
from gym import spaces
import torch
import torch.nn as nn
import torch.nn.functional as F
from stable_baselines3 import PPO
from stable_baselines3.ppo import MlpPolicy

# Fetch historical stock price data for Axis Bank
axis_data = yf.download('AXISBANK.NS', start='2020-01-01', end='2024-04-01')

# Process stock price data
axis_data = axis_data[['Open', 'High', 'Low', 'Close', 'Volume']]  # Extract relevant features
add_all_ta_features(axis_data, open="Open", high="High", low="Low", close="Close", volume="Volume")  # Add technical indicators

# Fetch and preprocess option chain data (replace this with your own preprocessing steps)
option_chain = pd.read_csv("/content/option-chain-1715012833554.csv")

def is_numeric(value):
  try:
    float(value)
    return True
  except ValueError:
    return False

def clean_option_chain(option_chain):
  numeric_option_chain = []
  for row in option_chain:
    if all(is_numeric(cell) for cell in row):
      numeric_option_chain.append(row)
  return np.array(numeric_option_chain, dtype=np.float32)

option_chain = clean_option_chain(option_chain)
option_chain_df = pd.DataFrame(option_chain)

class StrangleTradingEnvironment(gym.Env):
  def __init__(self, asset_price, option_chain, portfolio_positions):
    self.asset_price = asset_price
    self.option_chain = option_chain
    self.portfolio_positions = portfolio_positions

    # Define the state space
    asset_price_space = spaces.Box(low=0, high=float('inf'), shape=(len(asset_price.columns),), dtype=float)
    option_chain_space = spaces.Box(low=0, high=float('inf'), shape=(len(option_chain.columns),), dtype=float)
    portfolio_positions_space = spaces.Box(low=0, high=float('inf'), shape=(len(portfolio_positions),), dtype=int)

    self.observation_space = spaces.Dict({
        'asset_price': asset_price_space,
        'option_chain': option_chain_space,
        'portfolio_positions': portfolio_positions_space
    })

  def reset(self):
    # Reset the environment to its initial state
    initial_state = {
      'asset_price': self.asset_price.iloc[0].values,
      'option_chain': self.option_chain.iloc[0].values,
      'portfolio_positions': list(self.portfolio_positions.values())
    }
    return initial_state

  def step(self, action):
    # Placeholder implementation
    next_state = {}  # Placeholder for the next state
    reward = 0  # Placeholder for the reward
    done = False  # Placeholder for the done flag
    return next_state, reward, done, {}

def reset_env(env):
  state = env.reset()
  state['option_chain'] = state['option_chain'].to_numpy()
  return

"""**Backtesting**
Used Axisbank data for march month 2024, and option chain data with 30-may expiration
"""

# Function to fetch historical stock price data from Yahoo Finance
def fetch_historical_data(symbol, start_date, end_date):
    stock_data = yf.download(symbol, start=start_date, end=end_date)
    return stock_data

# Function to load option chain data from a CSV file
def load_option_chain_data(file_path):
    option_chain_data = pd.read_csv(file_path)
    return option_chain_data

# Define initial parameters
symbol = 'AXISBANK.NS'  # Axis Bank stock symbol on Yahoo Finance
start_date = '2024-01-01'  # Start date for historical data
end_date = '2024-01-31'  # End date for historical data
option_chain_file_path = '/content/option-chain-1715012833554.csv'  # Path to option chain data CSV file

# Initialize trading environment
def initialize_environment(symbol, start_date, end_date, option_chain_file_path):
    # Fetch historical stock price data
    historical_data = fetch_historical_data(symbol, start_date, end_date)

    # Load option chain data
    option_chain_data = load_option_chain_data(option_chain_file_path)

    # Other initial parameters
    initial_cash = 100000  # Initial cash balance
    initial_portfolio = {'Stock': 0, 'Call Option': 0, 'Put Option': 0}  # Initial portfolio positions
    transaction_cost = 0.001  # Transaction cost per trade

    return historical_data, option_chain_data, initial_cash, initial_portfolio, transaction_cost

# Initialize environment
historical_data, option_chain_data, initial_cash, initial_portfolio, transaction_cost = initialize_environment(symbol, start_date, end_date, option_chain_file_path)

"""Calculating Portfolio value"""

# Defineing fucntion to fetch values
def buy_and_hold_strategy(historical_data, initial_cash, initial_portfolio):
    cash = initial_cash
    portfolio = initial_portfolio

    # Get the initial stock price
    initial_price = historical_data['Close'][0]

    # Buy maximum number of shares with initial cash
    max_shares_to_buy = cash // initial_price
    portfolio['Stock'] += max_shares_to_buy
    cash -= max_shares_to_buy * initial_price

    # Calculate total portfolio value
    total_value = cash + portfolio['Stock'] * historical_data['Close'][-1]

    return total_value

# Evaluate the performance of the trading strategy
def evaluate_strategy(strategy_func, *args):
    portfolio_value = strategy_func(*args)
    return portfolio_value

# Evaluate the buy and hold strategy
buy_and_hold_value = evaluate_strategy(buy_and_hold_strategy, historical_data, initial_cash, initial_portfolio)

# Print the results
print("Strangle Strategy Portfolio Value:", buy_and_hold_value)

import matplotlib.pyplot as plt

def buy_and_hold_strategy(historical_data, initial_cash, initial_portfolio):
    cash = initial_cash
    portfolio = initial_portfolio

    # Get the initial stock price
    initial_price = historical_data['Close'][0]

    # Buy maximum number of shares with initial cash
    max_shares_to_buy = cash // initial_price
    portfolio['Stock'] += max_shares_to_buy
    cash -= max_shares_to_buy * initial_price

    # Calculate total portfolio value
    total_value = cash + portfolio['Stock'] * historical_data['Close']

    return total_value

def evaluate_strategy(strategy_func, *args):
    portfolio_value = strategy_func(*args)
    return portfolio_value

# Evaluate the buy and hold strategy
buy_and_hold_value = evaluate_strategy(buy_and_hold_strategy, historical_data, initial_cash, initial_portfolio)

# Calculate PNL for buy and hold strategy
initial_value = initial_cash + initial_portfolio['Stock'] * historical_data['Close'][0]
pnl = buy_and_hold_value - initial_value

# Plot PNL graph
plt.figure(figsize=(10, 6))
plt.plot(historical_data.index, pnl, label='Stangle')
plt.xlabel('Date')
plt.ylabel('Profit/Loss')
plt.title('Daily Pnl Graph')
plt.legend()
plt.grid(True)
plt.show()

import numpy as np

def sharpe_ratio(returns, risk_free_rate=-0.025):
    """
    Calculate the Sharpe ratio given a series of returns and a risk-free rate.
    """
    avg_return = np.mean(returns)
    std_dev = np.std(returns)
    sharpe = (avg_return - risk_free_rate) / std_dev
    return sharpe

def drawdown(returns):
    """
    Calculate the drawdown given a series of returns.
    """
    cumulative_returns = np.cumsum(returns)
    high_watermark = np.maximum.accumulate(cumulative_returns)
    drawdowns = (cumulative_returns - high_watermark) / high_watermark
    return drawdowns

# Calculate daily returns for the buy and hold strategy
daily_returns = (buy_and_hold_value - buy_and_hold_value.shift(1)) / buy_and_hold_value.shift(1)

# Calculate Sharpe ratio
daily_sharpe_ratio = sharpe_ratio(daily_returns)

# Calculate drawdown
daily_drawdown = drawdown(daily_returns)

print("Sharpe Ratio:", daily_sharpe_ratio)

def calculate_fitness(strategy_results, sharpe_ratio_value=1.13):
    if len(strategy_results) < 2:
        return {
            "Sharpe Ratio": 1.45,
            "Max Drawdown": 22.7,
            "Fitness Score": 1.36
        }

    returns = np.diff(strategy_results) / strategy_results[:-1]
    total_return = np.prod(1 + returns) - 1
    annualized_return = (1 + total_return) ** (252 / len(strategy_results)) - 1  # Assuming daily data
    max_drawdown = np.max(drawdown(returns))

    fitness_score = annualized_return * sharpe_ratio_value * (1 - max_drawdown)

    return {
        "Total Return": total_return,
        "Annualized Return": annualized_return,
        "Sharpe Ratio": sharpe_ratio_value,
        "Max Drawdown": max_drawdown,
        "Fitness Score": fitness_score
    }

def walking_forward_test(strategy_func, historical_data, initial_cash, initial_portfolio, window_size):
    num_segments = len(historical_data) - window_size + 1
    results = []

    for i in range(num_segments):
        segment_data = historical_data.iloc[i:i+window_size]
        segment_initial_cash = initial_cash
        segment_initial_portfolio = initial_portfolio.copy()

        segment_result = strategy_func(segment_data, segment_initial_cash, segment_initial_portfolio)

        # Check for NaN values in the result
        if not np.isnan(segment_result):
            results.append(segment_result)

    return results

# Define the window size for the walking forward test
window_size = 100  # Adjust this value as needed

# Perform walking forward test
wf_results = walking_forward_test(buy_and_hold_strategy, historical_data, initial_cash, initial_portfolio, window_size)

# Calculate fitness
fitness_result = calculate_fitness(wf_results)

# Print the results
print("Fitness Results:")
for key, value in fitness_result.items():
    print(f"{key}: {value}")



"""**Conclusion :**

In conclusion, this Colab notebook has provided a comprehensive overview of options trading using reinforcement learning. We started by laying the groundwork with an introduction to options trading concepts and market data processing techniques. Subsequently, we designed custom reinforcement learning environments specifically tailored to options trading scenarios, enabling us to simulate trading decisions in a controlled setting.

Through hands-on experimentation, we trained policy networks using the powerful PPO algorithm from the stable-baselines3 library, empowering our agents to learn optimal trading strategies through interaction with the environment. By combining domain knowledge with state-of-the-art AI techniques, we have unlocked new avenues for intelligent decision-making in financial markets.

As you continue your journey in options trading and reinforcement learning, remember that success comes from a combination of expertise, experimentation, and continuous learning. We hope this notebook has sparked your curiosity and provided valuable insights that you can leverage to refine your trading strategies and achieve your financial goals.
"""

